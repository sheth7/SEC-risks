{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('SEC': conda)",
   "display_name": "Python 3.7.4 64-bit ('SEC': conda)",
   "metadata": {
    "interpreter": {
     "hash": "bc403f9cf65ded6d699286fd6d13f06e3e4a86bcb71d6a2c80043385526cfdf1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import errno\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import re\n",
    "from tqdm import trange\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARQueryError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when a query receives a response that is not a 200 response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, response):\n",
    "        self.response = response\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"An error occured while making the query. Received {response} response\".format(\n",
    "            response=self.response\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARFieldError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when an invalid field is given to an endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint, field):\n",
    "        self.endpoint = endpoint\n",
    "        self.field = field\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Field {field} not found in endpoint {endpoint}\".format(\n",
    "            field=self.field, endpoint=self.endpoint\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIKError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when an invalid CIK is given.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cik):\n",
    "        self.cik = cik\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"CIK {cik} is not valid. Must be str or int with 10 digits.\".format(cik=self.cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_data_path = os.path.abspath(\"/Users/anan_mac/Projects/SEC-risks/SEC-company-data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"sample.txt\", \"r\") as f:\n",
    "#     sample = f.readlines()\n",
    "#     compile = re.findall(r\"(?<=TABLE OF CONTENTS)(.*)(?=PART I)\",sample[0])\n",
    "#     print(compile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = data.lower()  \n",
    "#    removing the \\t and \\n characters\n",
    "    # data = data.replace('\\t',' ').replace('\\n',' ')\n",
    "\n",
    "#    removing TABLE OF CONTENTS to PART I      \n",
    "    clean = re.compile(r\"(?<=TABLE OF CONTENTS)(.*)(?=PART I)\")\n",
    "    data = re.sub(clean,\" \",data)\n",
    "\n",
    "#    keeping the string between the two Item 1A. and Item 1B.   \n",
    "    data = re.findall(r\"item 1a\\.(.+?)item 1b\\.\",data)\n",
    "    data = \" \".join(data)\n",
    "\n",
    "    data = data.replace('&#8217;',\"'\").replace('&#39;', \"'\")\n",
    "\n",
    "#    removing non-breaking special characters\n",
    "    clean = re.compile(r\"&#\\w+;\")\n",
    "    data = re.sub(clean,' ', str(data))\n",
    "\n",
    "#    removing non-breaking special characters without ; at end       \n",
    "    clean = re.compile(r\"&#\\w+\")\n",
    "    data = re.sub(clean,' ', str(data))\n",
    "\n",
    "#    removing the non-breaking space and special characters    \n",
    "    # data = data.replace('&#160;',' ').replace('&nbsp;',' ')\n",
    "    # data = data.replace('&#174',' ').replace('&#xA0;',' ')\n",
    "    # data = data.replace('&#32;',' ').replace('&#8220;',' ')\n",
    "    # data = data.replace('&#8221;',' ').replace('&#8217;', ' ')\n",
    "    # data = data.replace('&#149',' ').replace('&#146',' ')\n",
    "    # data = data.replace('&#x',' ')\n",
    "\n",
    "\n",
    "#    converting list of strings (output of re.findall into single string)    \n",
    "    # data = \". \".join(data)\n",
    "                                                \n",
    "                        \n",
    "##    removing the <HEAD></HEAD> from utf-8 encoded data       \n",
    "#    clean = re.compile('<HEAD>.*?</HEAD>')\n",
    "#    data = re.sub(clean,' ', data)                        \n",
    "#\n",
    "##    removing the <TABLE></TABLE> from utf-8 encoded data       \n",
    "#    clean = re.compile('<CENTER>.*?</CENTER>')\n",
    "#    data = re.sub(clean,' ', data)                        \n",
    "#\n",
    "##    removing the image utf-8 encoded data       \n",
    "#    clean = re.compile('\\.jpg(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "#\n",
    "##    removing the zip utf-8 encoded data\n",
    "#    clean = re.compile('\\.zip(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)    \n",
    "#    \n",
    "##    removing the xls utf-8 encoded data\n",
    "#    clean = re.compile('\\.xls(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "#\n",
    "##    removing the xsd utf-8 encoded data\n",
    "#    clean = re.compile('\\.xsd(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)    \n",
    "#\n",
    "#    \n",
    "##    removing the png utf-8 encoded data\n",
    "#    clean = re.compile('\\.png.*|\\s*<TEXT>.*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)   \n",
    "#    \n",
    "##    removing the pdf utf-8 encoded data\n",
    "#    clean = re.compile('\\.pdf(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "\n",
    "##    removing the XBRL format utf-8 encoded data   \n",
    "#    clean = re.compile('<XBRL>(.|\\s)*?</XBRL>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "#\n",
    "#\n",
    "##    removing the .htm format utf-8 encoded data        \n",
    "#    clean = re.compile('\\.htm(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "#\n",
    "#\n",
    "##    removing the xml utf-8 encoded data\n",
    "#    clean = re.compile('\\.xml(.|\\s)*?</TEXT>')\n",
    "#    data = re.sub(clean,' ', data)\n",
    "\n",
    "#    removing the \\t and \\n characters\n",
    "#    data = path.replace('\\t',' ').replace('\\n',' ')\n",
    "#    data = path.replace('\\t',' ')\n",
    "\n",
    "#    removing the html tags from data\n",
    "\n",
    "    clean = re.compile(r'<(.|\\s)*?>')\n",
    "    data = re.sub(clean,' ', data)\n",
    "\n",
    "    data = \" \".join(data.strip().split())\n",
    "\n",
    "#    clean4 = re.compile('<(.|\\s)*?>')\n",
    "#    data4 = re.sub(clean4,' ', data3)\n",
    "\n",
    "\n",
    "#    removing all numbers from data   - this seems like a bad idea\n",
    "\n",
    "#    clean4 = re.compile('\\d+(?:\\.\\d+)?')\n",
    "#    data4 = re.sub(clean4, ' ', data3)\n",
    "    \n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SecCrawler(object):\n",
    "\n",
    "    def __init__(self, data_path=default_data_path):\n",
    "        self.data_path = data_path\n",
    "        print(\"Directory where reports are stored:  \" + self.data_path)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SecCrawler(data_path{0})\".format(self.data_path)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SecCrawler(data_path{0})\".format(self.data_path)\n",
    "\n",
    "\n",
    "    def _make_directory(self, company_code, cik, priorto, filing_type):\n",
    "        # path = os.path.join(self.data_path, company_code, cik, filing_type)\n",
    "        path = os.path.join(self.data_path, company_code)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError as Exception:\n",
    "                if Exception.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "\n",
    "    def _save_in_directory(self, company_code, cik, priorto, filing_type, docs):\n",
    "        # Save every text document into its respective folder\n",
    "        for (url, doc_name) in docs:\n",
    "            r = requests.get(url)\n",
    "            data = r.text\n",
    "            # data1 = clean_data(data)\n",
    "            data1 = data \n",
    "                       \n",
    "            # path = os.path.join(self.data_path, company_code, cik,\n",
    "            #                     filing_type, doc_name)\n",
    "            path = os.path.join(self.data_path, company_code, doc_name)\n",
    "\n",
    "            with open(path, \"ab\") as f:\n",
    "                f.write(data1.encode('ascii', 'ignore'))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_document_list(data):\n",
    "        # parse fetched data using beautifulsoup\n",
    "        # Explicit parser needed\n",
    "        soup = BeautifulSoup(data, features='html.parser')\n",
    "        # store the link in the list\n",
    "        link_list = [link.string for link in soup.find_all('filinghref')]\n",
    "\n",
    "        print(\"Number of files to download: {0}\".format(len(link_list)))\n",
    "        print(\"Starting download...\")\n",
    "\n",
    "        # List of url to the text documents\n",
    "        txt_urls = [link[:link.rfind(\"-\")] + \".txt\" for link in link_list]\n",
    "        # List of document doc_names\n",
    "        doc_names = [url.split(\"/\")[-1] for url in txt_urls]\n",
    "\n",
    "        return list(zip(txt_urls, doc_names))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_date(date):\n",
    "        if isinstance(date, datetime.datetime):\n",
    "            return date.strftime(\"%Y%m%d\")\n",
    "        elif isinstance(date, str):\n",
    "            if len(date) != 8:\n",
    "                raise TypeError('Date must be of the form YYYYMMDD')\n",
    "        elif isinstance(date, int):\n",
    "            if date < 10**7 or date > 10**8:\n",
    "                raise TypeError('Date must be of the form YYYYMMDD')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_cik(cik):\n",
    "        invalid_str = isinstance(cik, str) and len(cik) != 10\n",
    "        invalid_int = isinstance(cik, int) and not (999999999 < cik < 10**10)\n",
    "        invalid_type = not isinstance(cik, (int, str))\n",
    "        if invalid_str or invalid_int or invalid_type:\n",
    "            raise CIKError(cik)\n",
    "        else:\n",
    "            return cik\n",
    "\n",
    "    def _fetch_report(self, company_code, cik, priorto, count, filing_type):\n",
    "        priorto = self._sanitize_date(priorto)\n",
    "        cik = self._check_cik(cik)\n",
    "        self._make_directory(company_code, cik, priorto, filing_type)\n",
    "\n",
    "        # generate the url to crawl\n",
    "        base_url = \"http://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        params = {'action': 'getcompany', 'owner': 'exclude', 'output': 'xml',\n",
    "                  'CIK': cik, 'type': filing_type, 'dateb': priorto, 'count': count}\n",
    "        print(\"started {filing_type} {company_code}\".format(\n",
    "            filing_type=filing_type, company_code=company_code))\n",
    "        r = requests.get(base_url, params=params)\n",
    "        if r.status_code == 200:\n",
    "            data = r.text\n",
    "#            tree = html.fromstring(r.content)\n",
    "            # get doc list data\n",
    "            docs = self._create_document_list(data)\n",
    "\n",
    "            try:\n",
    "                self._save_in_directory(\n",
    "                    company_code, cik, priorto, filing_type, docs)\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        else:\n",
    "            raise EDGARQueryError(r.status_code)\n",
    "\n",
    "            \n",
    "        \n",
    "    def filing_10Q(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '10-Q')\n",
    "        return path\n",
    "\n",
    "    def filing_10K(self, company_code, cik, priorto, count):\n",
    "#        path = self._fetch_report(company_code, cik, priorto, count, '10-K')\n",
    "#        return path\n",
    "        self._fetch_report(company_code, cik, priorto, count, '10-K')\n",
    "\n",
    "    def filing_8K(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '8-K')\n",
    "        return path\n",
    "\n",
    "    def filing_13F(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '13-F')\n",
    "        return path\n",
    "\n",
    "    def filing_SD(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, 'SD')\n",
    "        return path\n",
    "\n",
    "    def filing_4(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '4')\n",
    "        return path\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filings(a,b,c,d):\n",
    "    t1 = time.time()\n",
    "    seccrawler = SecCrawler() # creating object crawler from class SecCrawler()\n",
    "    \n",
    "    companyCode = a    #company code for Apple Inc\n",
    "    cik = b      #cik code for Apple Inc\n",
    "    date = c       #date from which filings should be downloaded\n",
    "    count = d            # number of filings to be downloaded, at minimum 10 entries by EDGAR\n",
    "    \n",
    "#   Crawling, creating consolidated file, returning path to consolidated file\n",
    "#    path = seccrawler.filing_10K(companyCode, cik, date, count)\n",
    "    seccrawler.filing_10K(companyCode, cik, date, count)\n",
    "\n",
    "    print(\"Successfully downloaded all the files\")\n",
    "    \n",
    "#   Clocking out \n",
    "    t2 = time.time()\n",
    "    print(\"Total time taken: {0}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Directory where reports are stored:  /Users/anan_mac/Projects/SEC-risks/SEC-company-data/test\n",
      "started 10-K AAPL\n",
      "Number of files to download: 20\n",
      "Starting download...\n",
      "Successfully downloaded all the files\n",
      "Total time taken: 8.658684253692627\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    companyCode = ['AAPL']\n",
    "    cik = ['0000320193']\n",
    "    \n",
    "    # with open('sp500.txt') as f:\n",
    "    #     df = pd.read_csv(f,sep=',')\n",
    "    #     df[\"CIK\"] = df.CIK.map(\"{:010}\".format)\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         companyCode.append(row['Name'])\n",
    "    #         cik.append(row['CIK'])\n",
    "    \n",
    "#    with open('cik_ticker_database.csv') as f:\n",
    "#        df = pd.read_csv(f, sep='|')\n",
    "#        df[\"CIK\"] = df.CIK.map(\"{:010}\".format)\n",
    "#        for index, row in df.iterrows():\n",
    "#            companyCode.append(row['Name'])\n",
    "#            cik.append(row['CIK'])\n",
    "\n",
    "\n",
    "    # companyCode = ['Norfolk Southern','Southwest Airlines','International Paper'\n",
    "    #               ,'PG&E','Freeport-McMoRan','Bristol-Myers Squibb'\n",
    "    #               ,'Texas Instruments','Las Vegas Sands','Las Vegas Sands b'\n",
    "    #               ,'Abbott Laboratories','Marriott International','Biogen'\n",
    "    #               ,'Monsanto','Andeavor','AmerisourceBergen','Applied Materials'\n",
    "    #               ,'General Motors','Cisco Systems','TJX Cos'\n",
    "    #               ,'American International Group']    #company code for Apple Inc\n",
    "    # cik = ['0000702165','0000092380','0000051434','0001004980','0000831259'\n",
    "    #       ,'0000014272','0000097476','0001300514','0000850994' ,'0001441848'\n",
    "    #       ,'0001048286','0000875045','0001110783','0000050104','0001140859'\n",
    "    #       ,'0000006951','0000040730','0000858877','0000109198','0000005272']      #cik code for Apple Inc\n",
    "    date = '20201231'       #date from which filings should be downloaded\n",
    "    count = 25            # number of filings to be downloaded, at minimum 10 entries by EDGAR\n",
    "    \n",
    "    for i in range(len(cik)):\n",
    "#        path = get_filings(companyCode[i], cik[i], date, count)     #Fetching the data based on input details\n",
    "        get_filings(companyCode[i], cik[i], date, count)     #Fetching the data based on input details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}