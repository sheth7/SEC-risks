{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('SEC': conda)",
   "metadata": {
    "interpreter": {
     "hash": "bc403f9cf65ded6d699286fd6d13f06e3e4a86bcb71d6a2c80043385526cfdf1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import errno\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import re\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARQueryError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when a query receives a response that is not a 200 response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, response):\n",
    "        self.response = response\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"An error occured while making the query. Received {response} response\".format(\n",
    "            response=self.response\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARFieldError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when an invalid field is given to an endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint, field):\n",
    "        self.endpoint = endpoint\n",
    "        self.field = field\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Field {field} not found in endpoint {endpoint}\".format(\n",
    "            field=self.field, endpoint=self.endpoint\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIKError(Exception):\n",
    "    \"\"\"\n",
    "    This error is thrown when an invalid CIK is given.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cik):\n",
    "        self.cik = cik\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"CIK {cik} is not valid. Must be str or int with 10 digits.\".format(cik=self.cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_data_path = os.path.abspath(\"/Users/anan_mac/Projects/SEC-risks/SEC-company-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = data.lower()  \n",
    "\n",
    "#    removing the html tags from data\n",
    "    clean = re.compile(r'<(.|\\s)*?>')\n",
    "    new_data = re.sub(clean,' ', data)\n",
    "\n",
    "    new_data = new_data.replace('&#8217;',\"'\").replace('&#39;', \"'\")\n",
    "\n",
    "#    removing the &#; from data\n",
    "    clean = re.compile(r'&#.*?;')\n",
    "    new_data = re.sub(clean,' ', new_data)\n",
    "\n",
    "#    removing the &nbsp; from data\n",
    "    clean = re.compile(r'&nbsp;')\n",
    "    new_data = re.sub(clean,' ', new_data)\n",
    "\n",
    "#    keeping the string between the two Item 1A. and Item 1B.   \n",
    "    data1 = re.findall(r\"item 1a\\.(.+?)item 1b\\.\",new_data, re.S)\n",
    "    new_data1 = \" \".join(data1)\n",
    "\n",
    "#    keeping the string between the two Item 7. and Item 8.   \n",
    "    data2 = re.findall(r\"item 7a\\.(.+?)item 8\\.\",new_data, re.S)\n",
    "    new_data2 = \" \".join(data2)\n",
    "\n",
    "    new_data = new_data1 + new_data2\n",
    "\n",
    "#    removing the \\ss\\s from data\n",
    "    clean = re.compile(r'\\ss\\s')\n",
    "    new_data = re.sub(clean,' ', new_data)\n",
    "\n",
    "    new_data = \" \".join(new_data.strip().split())\n",
    "    \n",
    "    return (new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SecCrawler(object):\n",
    "\n",
    "    def __init__(self, data_path=default_data_path):\n",
    "        self.data_path = data_path\n",
    "        print(\"Directory where reports are stored:  \" + self.data_path)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SecCrawler(data_path{0})\".format(self.data_path)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SecCrawler(data_path{0})\".format(self.data_path)\n",
    "\n",
    "\n",
    "    def _make_directory(self, company_code, cik, priorto, filing_type):\n",
    "        # path = os.path.join(self.data_path, company_code, cik, filing_type)\n",
    "        path = os.path.join(self.data_path, company_code)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError as Exception:\n",
    "                if Exception.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "\n",
    "    def _save_in_directory(self, company_code, cik, priorto, filing_type, docs):\n",
    "        # Save every text document into its respective folder\n",
    "        for (url, doc_name) in docs:\n",
    "            r = requests.get(url)\n",
    "            data = r.text\n",
    "            data1 = clean_data(data)\n",
    "            # data1 = data \n",
    "\n",
    "            # path = os.path.join(self.data_path, company_code, cik,\n",
    "            #                     filing_type, doc_name)\n",
    "            path = os.path.join(self.data_path, company_code, doc_name)\n",
    "\n",
    "            with open(path, \"ab\") as f:\n",
    "                f.write(data1.encode('ascii', 'ignore'))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_document_list(data):\n",
    "        # parse fetched data using beautifulsoup\n",
    "        # Explicit parser needed\n",
    "        soup = BeautifulSoup(data, features='html.parser')\n",
    "        # store the link in the list\n",
    "        link_list = [link.string for link in soup.find_all('filinghref')]\n",
    "\n",
    "        print(\"Number of files to download: {0}\".format(len(link_list)))\n",
    "        print(\"Starting download...\")\n",
    "\n",
    "        # List of url to the text documents\n",
    "        txt_urls = [link[:link.rfind(\"-\")] + \".txt\" for link in link_list]\n",
    "        # List of document doc_names\n",
    "        doc_names = [url.split(\"/\")[-1] for url in txt_urls]\n",
    "\n",
    "        return list(zip(txt_urls, doc_names))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_date(date):\n",
    "        if isinstance(date, datetime.datetime):\n",
    "            return date.strftime(\"%Y%m%d\")\n",
    "        elif isinstance(date, str):\n",
    "            if len(date) != 8:\n",
    "                raise TypeError('Date must be of the form YYYYMMDD')\n",
    "        elif isinstance(date, int):\n",
    "            if date < 10**7 or date > 10**8:\n",
    "                raise TypeError('Date must be of the form YYYYMMDD')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_cik(cik):\n",
    "        invalid_str = isinstance(cik, str) and len(cik) != 10\n",
    "        invalid_int = isinstance(cik, int) and not (999999999 < cik < 10**10)\n",
    "        invalid_type = not isinstance(cik, (int, str))\n",
    "        if invalid_str or invalid_int or invalid_type:\n",
    "            raise CIKError(cik)\n",
    "        else:\n",
    "            return cik\n",
    "\n",
    "    def _fetch_report(self, company_code, cik, priorto, count, filing_type):\n",
    "        priorto = self._sanitize_date(priorto)\n",
    "        cik = self._check_cik(cik)\n",
    "        self._make_directory(company_code, cik, priorto, filing_type)\n",
    "\n",
    "        # generate the url to crawl\n",
    "        base_url = \"http://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        params = {'action': 'getcompany', 'owner': 'exclude', 'output': 'xml',\n",
    "                  'CIK': cik, 'type': filing_type, 'dateb': priorto, 'count': count}\n",
    "        print(\"started {filing_type} {company_code}\".format(\n",
    "            filing_type=filing_type, company_code=company_code))\n",
    "        r = requests.get(base_url, params=params)\n",
    "        if r.status_code == 200:\n",
    "            data = r.text\n",
    "#            tree = html.fromstring(r.content)\n",
    "            # get doc list data\n",
    "            docs = self._create_document_list(data)\n",
    "\n",
    "            try:\n",
    "                self._save_in_directory(\n",
    "                    company_code, cik, priorto, filing_type, docs)\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        else:\n",
    "            raise EDGARQueryError(r.status_code)\n",
    "\n",
    "            \n",
    "        \n",
    "    def filing_10Q(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '10-Q')\n",
    "        return path\n",
    "\n",
    "    def filing_10K(self, company_code, cik, priorto, count):\n",
    "#        path = self._fetch_report(company_code, cik, priorto, count, '10-K')\n",
    "#        return path\n",
    "        self._fetch_report(company_code, cik, priorto, count, '10-K')\n",
    "\n",
    "    def filing_8K(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '8-K')\n",
    "        return path\n",
    "\n",
    "    def filing_13F(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '13-F')\n",
    "        return path\n",
    "\n",
    "    def filing_SD(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, 'SD')\n",
    "        return path\n",
    "\n",
    "    def filing_4(self, company_code, cik, priorto, count):\n",
    "        path = self._fetch_report(company_code, cik, priorto, count, '4')\n",
    "        return path\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filings(a,b,c,d):\n",
    "    t1 = time.time()\n",
    "    seccrawler = SecCrawler() # creating object crawler from class SecCrawler()\n",
    "    \n",
    "    companyCode = a    #company code for Apple Inc\n",
    "    cik = b      #cik code for Apple Inc\n",
    "    date = c       #date from which filings should be downloaded\n",
    "    count = d            # number of filings to be downloaded, at minimum 10 entries by EDGAR\n",
    "    \n",
    "#   Crawling, creating consolidated file, returning path to consolidated file\n",
    "#    path = seccrawler.filing_10K(companyCode, cik, date, count)\n",
    "    seccrawler.filing_10K(companyCode, cik, date, count)\n",
    "\n",
    "    print(\"Successfully downloaded all the files\")\n",
    "    \n",
    "#   Clocking out \n",
    "    t2 = time.time()\n",
    "    print(\"Total time taken: {0}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Directory where reports are stored:  /Users/anan_mac/Projects/SEC-risks/SEC-company-data\n",
      "started 10-K AMD\n",
      "Number of files to download: 20\n",
      "Starting download...\n",
      "Successfully downloaded all the files\n",
      "Total time taken: 21.49868416786194\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # companyCode = ['AMD']\n",
    "    # cik = ['0000002488']\n",
    "    \n",
    "    with open('sp500.txt') as f:\n",
    "        df = pd.read_csv(f,sep=',')\n",
    "        df[\"CIK\"] = df.CIK.map(\"{:010}\".format)\n",
    "        for index, row in df.iterrows():\n",
    "            companyCode.append(row['Name'])\n",
    "            cik.append(row['CIK'])\n",
    "    \n",
    "#    with open('cik_ticker_database.csv') as f:\n",
    "#        df = pd.read_csv(f, sep='|')\n",
    "#        df[\"CIK\"] = df.CIK.map(\"{:010}\".format)\n",
    "#        for index, row in df.iterrows():\n",
    "#            companyCode.append(row['Name'])\n",
    "#            cik.append(row['CIK'])\n",
    "\n",
    "\n",
    "    # companyCode = ['Norfolk Southern','Southwest Airlines','International Paper'\n",
    "    #               ,'PG&E','Freeport-McMoRan','Bristol-Myers Squibb'\n",
    "    #               ,'Texas Instruments','Las Vegas Sands','Las Vegas Sands b'\n",
    "    #               ,'Abbott Laboratories','Marriott International','Biogen'\n",
    "    #               ,'Monsanto','Andeavor','AmerisourceBergen','Applied Materials'\n",
    "    #               ,'General Motors','Cisco Systems','TJX Cos'\n",
    "    #               ,'American International Group']    #company code for Apple Inc\n",
    "    # cik = ['0000702165','0000092380','0000051434','0001004980','0000831259'\n",
    "    #       ,'0000014272','0000097476','0001300514','0000850994' ,'0001441848'\n",
    "    #       ,'0001048286','0000875045','0001110783','0000050104','0001140859'\n",
    "    #       ,'0000006951','0000040730','0000858877','0000109198','0000005272']      #cik code for Apple Inc\n",
    "    date = '20201231'       #date from which filings should be downloaded\n",
    "    count = 25            # number of filings to be downloaded, at minimum 10 entries by EDGAR\n",
    "    \n",
    "    for i in range(len(cik)):\n",
    "#        path = get_filings(companyCode[i], cik[i], date, count)     #Fetching the data based on input details\n",
    "        get_filings(companyCode[i], cik[i], date, count)     #Fetching the data based on input details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}